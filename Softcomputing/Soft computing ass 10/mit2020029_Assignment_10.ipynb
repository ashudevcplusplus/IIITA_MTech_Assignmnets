{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "SMO_final.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BhLD7H78JU5"
      },
      "source": [
        "# **IMPORTING LIBRARIES AND FETCHING DATA SET**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "So4SDHs67-i6",
        "outputId": "852ecd79-2d6c-4ac5-fee9-ef4e85ccc244"
      },
      "source": [
        "import numpy as IMORTED____lib___numpy\n",
        "import matplotlib.pyplot as IMORTED____lib___plot\n",
        "import random\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import preprocessing\n",
        "\n",
        "train = open(\"hd.txt\",\"r\")\n",
        "X = IMORTED____lib___numpy.ndarray(shape=(303, 2), dtype=float)\n",
        "Y = IMORTED____lib___numpy.ndarray(shape=(303, ), dtype=float)\n",
        "i = 0\n",
        "for line in train:\n",
        "    col = line.split(',')\n",
        "    X[i][0] = col[0]        \n",
        "    X[i][1] = col[3]\n",
        "    Y[i] = col[13].split('\\n')[0]\n",
        "    if(Y[i] == 0):\n",
        "        Y[i] = -1\n",
        "    else:\n",
        "        Y[i] = 1\n",
        "    i += 1\n",
        "print(X)\n",
        "print(Y)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 63. 145.]\n",
            " [ 67. 160.]\n",
            " [ 67. 120.]\n",
            " [ 37. 130.]\n",
            " [ 41. 130.]\n",
            " [ 56. 120.]\n",
            " [ 62. 140.]\n",
            " [ 57. 120.]\n",
            " [ 63. 130.]\n",
            " [ 53. 140.]\n",
            " [ 57. 140.]\n",
            " [ 56. 140.]\n",
            " [ 56. 130.]\n",
            " [ 44. 120.]\n",
            " [ 52. 172.]\n",
            " [ 57. 150.]\n",
            " [ 48. 110.]\n",
            " [ 54. 140.]\n",
            " [ 48. 130.]\n",
            " [ 49. 130.]\n",
            " [ 64. 110.]\n",
            " [ 58. 150.]\n",
            " [ 58. 120.]\n",
            " [ 58. 132.]\n",
            " [ 60. 130.]\n",
            " [ 50. 120.]\n",
            " [ 58. 120.]\n",
            " [ 66. 150.]\n",
            " [ 43. 150.]\n",
            " [ 40. 110.]\n",
            " [ 69. 140.]\n",
            " [ 60. 117.]\n",
            " [ 64. 140.]\n",
            " [ 59. 135.]\n",
            " [ 44. 130.]\n",
            " [ 42. 140.]\n",
            " [ 43. 120.]\n",
            " [ 57. 150.]\n",
            " [ 55. 132.]\n",
            " [ 61. 150.]\n",
            " [ 65. 150.]\n",
            " [ 40. 140.]\n",
            " [ 71. 160.]\n",
            " [ 59. 150.]\n",
            " [ 61. 130.]\n",
            " [ 58. 112.]\n",
            " [ 51. 110.]\n",
            " [ 50. 150.]\n",
            " [ 65. 140.]\n",
            " [ 53. 130.]\n",
            " [ 41. 105.]\n",
            " [ 65. 120.]\n",
            " [ 44. 112.]\n",
            " [ 44. 130.]\n",
            " [ 60. 130.]\n",
            " [ 54. 124.]\n",
            " [ 50. 140.]\n",
            " [ 41. 110.]\n",
            " [ 54. 125.]\n",
            " [ 51. 125.]\n",
            " [ 51. 130.]\n",
            " [ 46. 142.]\n",
            " [ 58. 128.]\n",
            " [ 54. 135.]\n",
            " [ 54. 120.]\n",
            " [ 60. 145.]\n",
            " [ 60. 140.]\n",
            " [ 54. 150.]\n",
            " [ 59. 170.]\n",
            " [ 46. 150.]\n",
            " [ 65. 155.]\n",
            " [ 67. 125.]\n",
            " [ 62. 120.]\n",
            " [ 65. 110.]\n",
            " [ 44. 110.]\n",
            " [ 65. 160.]\n",
            " [ 60. 125.]\n",
            " [ 51. 140.]\n",
            " [ 48. 130.]\n",
            " [ 58. 150.]\n",
            " [ 45. 104.]\n",
            " [ 53. 130.]\n",
            " [ 39. 140.]\n",
            " [ 68. 180.]\n",
            " [ 52. 120.]\n",
            " [ 44. 140.]\n",
            " [ 47. 138.]\n",
            " [ 53. 128.]\n",
            " [ 53. 138.]\n",
            " [ 51. 130.]\n",
            " [ 66. 120.]\n",
            " [ 62. 160.]\n",
            " [ 62. 130.]\n",
            " [ 44. 108.]\n",
            " [ 63. 135.]\n",
            " [ 52. 128.]\n",
            " [ 59. 110.]\n",
            " [ 60. 150.]\n",
            " [ 52. 134.]\n",
            " [ 48. 122.]\n",
            " [ 45. 115.]\n",
            " [ 34. 118.]\n",
            " [ 57. 128.]\n",
            " [ 71. 110.]\n",
            " [ 49. 120.]\n",
            " [ 54. 108.]\n",
            " [ 59. 140.]\n",
            " [ 57. 128.]\n",
            " [ 61. 120.]\n",
            " [ 39. 118.]\n",
            " [ 61. 145.]\n",
            " [ 56. 125.]\n",
            " [ 52. 118.]\n",
            " [ 43. 132.]\n",
            " [ 62. 130.]\n",
            " [ 41. 135.]\n",
            " [ 58. 140.]\n",
            " [ 35. 138.]\n",
            " [ 63. 130.]\n",
            " [ 65. 135.]\n",
            " [ 48. 130.]\n",
            " [ 63. 150.]\n",
            " [ 51. 100.]\n",
            " [ 55. 140.]\n",
            " [ 65. 138.]\n",
            " [ 45. 130.]\n",
            " [ 56. 200.]\n",
            " [ 54. 110.]\n",
            " [ 44. 120.]\n",
            " [ 62. 124.]\n",
            " [ 54. 120.]\n",
            " [ 51.  94.]\n",
            " [ 29. 130.]\n",
            " [ 51. 140.]\n",
            " [ 43. 122.]\n",
            " [ 55. 135.]\n",
            " [ 70. 145.]\n",
            " [ 62. 120.]\n",
            " [ 35. 120.]\n",
            " [ 51. 125.]\n",
            " [ 59. 140.]\n",
            " [ 59. 170.]\n",
            " [ 52. 128.]\n",
            " [ 64. 125.]\n",
            " [ 58. 105.]\n",
            " [ 47. 108.]\n",
            " [ 57. 165.]\n",
            " [ 41. 112.]\n",
            " [ 45. 128.]\n",
            " [ 60. 102.]\n",
            " [ 52. 152.]\n",
            " [ 42. 102.]\n",
            " [ 67. 115.]\n",
            " [ 55. 160.]\n",
            " [ 64. 120.]\n",
            " [ 70. 130.]\n",
            " [ 51. 140.]\n",
            " [ 58. 125.]\n",
            " [ 60. 140.]\n",
            " [ 68. 118.]\n",
            " [ 46. 101.]\n",
            " [ 77. 125.]\n",
            " [ 54. 110.]\n",
            " [ 58. 100.]\n",
            " [ 48. 124.]\n",
            " [ 57. 132.]\n",
            " [ 52. 138.]\n",
            " [ 54. 132.]\n",
            " [ 35. 126.]\n",
            " [ 45. 112.]\n",
            " [ 70. 160.]\n",
            " [ 53. 142.]\n",
            " [ 59. 174.]\n",
            " [ 62. 140.]\n",
            " [ 64. 145.]\n",
            " [ 57. 152.]\n",
            " [ 52. 108.]\n",
            " [ 56. 132.]\n",
            " [ 43. 130.]\n",
            " [ 53. 130.]\n",
            " [ 48. 124.]\n",
            " [ 56. 134.]\n",
            " [ 42. 148.]\n",
            " [ 59. 178.]\n",
            " [ 60. 158.]\n",
            " [ 63. 140.]\n",
            " [ 42. 120.]\n",
            " [ 66. 160.]\n",
            " [ 54. 192.]\n",
            " [ 69. 140.]\n",
            " [ 50. 129.]\n",
            " [ 51. 140.]\n",
            " [ 43. 132.]\n",
            " [ 62. 138.]\n",
            " [ 68. 120.]\n",
            " [ 67. 100.]\n",
            " [ 69. 160.]\n",
            " [ 45. 138.]\n",
            " [ 50. 120.]\n",
            " [ 59. 160.]\n",
            " [ 50. 110.]\n",
            " [ 64. 180.]\n",
            " [ 57. 150.]\n",
            " [ 64. 140.]\n",
            " [ 43. 110.]\n",
            " [ 45. 142.]\n",
            " [ 58. 128.]\n",
            " [ 50. 144.]\n",
            " [ 55. 130.]\n",
            " [ 62. 150.]\n",
            " [ 37. 120.]\n",
            " [ 38. 120.]\n",
            " [ 41. 130.]\n",
            " [ 66. 178.]\n",
            " [ 52. 112.]\n",
            " [ 56. 120.]\n",
            " [ 46. 105.]\n",
            " [ 46. 138.]\n",
            " [ 64. 130.]\n",
            " [ 59. 138.]\n",
            " [ 41. 112.]\n",
            " [ 54. 108.]\n",
            " [ 39.  94.]\n",
            " [ 53. 123.]\n",
            " [ 63. 108.]\n",
            " [ 34. 118.]\n",
            " [ 47. 112.]\n",
            " [ 67. 152.]\n",
            " [ 54. 110.]\n",
            " [ 66. 112.]\n",
            " [ 52. 136.]\n",
            " [ 55. 180.]\n",
            " [ 49. 118.]\n",
            " [ 74. 120.]\n",
            " [ 54. 160.]\n",
            " [ 54. 122.]\n",
            " [ 56. 130.]\n",
            " [ 46. 120.]\n",
            " [ 49. 134.]\n",
            " [ 42. 120.]\n",
            " [ 41. 110.]\n",
            " [ 41. 126.]\n",
            " [ 49. 130.]\n",
            " [ 61. 134.]\n",
            " [ 60. 120.]\n",
            " [ 67. 120.]\n",
            " [ 58. 100.]\n",
            " [ 47. 110.]\n",
            " [ 52. 125.]\n",
            " [ 62. 128.]\n",
            " [ 57. 110.]\n",
            " [ 58. 146.]\n",
            " [ 64. 128.]\n",
            " [ 51. 120.]\n",
            " [ 43. 115.]\n",
            " [ 42. 120.]\n",
            " [ 67. 106.]\n",
            " [ 76. 140.]\n",
            " [ 70. 156.]\n",
            " [ 57. 124.]\n",
            " [ 44. 118.]\n",
            " [ 58. 136.]\n",
            " [ 60. 150.]\n",
            " [ 44. 120.]\n",
            " [ 61. 138.]\n",
            " [ 42. 136.]\n",
            " [ 52. 128.]\n",
            " [ 59. 126.]\n",
            " [ 40. 152.]\n",
            " [ 42. 130.]\n",
            " [ 61. 140.]\n",
            " [ 66. 160.]\n",
            " [ 46. 140.]\n",
            " [ 71. 112.]\n",
            " [ 59. 134.]\n",
            " [ 64. 170.]\n",
            " [ 66. 146.]\n",
            " [ 39. 138.]\n",
            " [ 57. 154.]\n",
            " [ 58. 130.]\n",
            " [ 57. 110.]\n",
            " [ 47. 130.]\n",
            " [ 55. 128.]\n",
            " [ 35. 122.]\n",
            " [ 61. 148.]\n",
            " [ 58. 114.]\n",
            " [ 58. 170.]\n",
            " [ 58. 125.]\n",
            " [ 56. 130.]\n",
            " [ 56. 120.]\n",
            " [ 67. 152.]\n",
            " [ 55. 132.]\n",
            " [ 44. 120.]\n",
            " [ 63. 140.]\n",
            " [ 63. 124.]\n",
            " [ 41. 120.]\n",
            " [ 59. 164.]\n",
            " [ 57. 140.]\n",
            " [ 45. 110.]\n",
            " [ 68. 144.]\n",
            " [ 57. 130.]\n",
            " [ 57. 130.]\n",
            " [ 38. 138.]]\n",
            "[-1.  1.  1. -1. -1. -1.  1. -1.  1.  1. -1. -1.  1. -1. -1. -1.  1. -1.\n",
            " -1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1.  1. -1.  1.  1. -1. -1. -1.\n",
            "  1.  1.  1. -1.  1. -1. -1. -1.  1.  1. -1.  1. -1. -1. -1. -1.  1. -1.\n",
            "  1.  1.  1.  1. -1. -1.  1. -1.  1. -1.  1.  1.  1. -1.  1.  1. -1.  1.\n",
            "  1.  1.  1. -1.  1. -1. -1.  1. -1. -1. -1.  1. -1. -1. -1. -1. -1. -1.\n",
            " -1.  1. -1. -1. -1.  1.  1.  1. -1. -1. -1. -1. -1. -1.  1. -1.  1.  1.\n",
            "  1.  1.  1.  1. -1.  1.  1. -1. -1. -1.  1.  1.  1.  1. -1.  1.  1. -1.\n",
            "  1.  1. -1. -1. -1. -1. -1. -1. -1. -1.  1.  1.  1. -1. -1.  1. -1.  1.\n",
            " -1.  1.  1. -1. -1. -1. -1. -1. -1.  1.  1.  1.  1.  1.  1. -1. -1.  1.\n",
            " -1. -1. -1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1. -1.  1. -1. -1.\n",
            "  1.  1. -1. -1.  1. -1. -1.  1.  1.  1. -1.  1.  1.  1. -1.  1. -1. -1.\n",
            " -1.  1. -1. -1. -1. -1. -1.  1.  1.  1. -1.  1. -1.  1. -1.  1.  1. -1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1.  1. -1. -1. -1.  1.  1. -1.  1.  1. -1.\n",
            " -1.  1.  1.  1. -1. -1. -1. -1. -1.  1. -1.  1.  1.  1.  1. -1. -1.  1.\n",
            " -1. -1. -1. -1. -1. -1. -1.  1. -1.  1. -1. -1.  1.  1.  1.  1.  1. -1.\n",
            "  1. -1.  1. -1.  1. -1. -1. -1.  1. -1.  1. -1.  1. -1.  1.  1.  1. -1.\n",
            " -1. -1.  1. -1.  1.  1.  1. -1.  1.  1.  1.  1.  1.  1. -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCisKt4D8gOJ"
      },
      "source": [
        "# **NORMALIZATION AND DATA SET SPLITTING**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKchuHfF7-li",
        "outputId": "a97c2f53-ce95-488d-e615-561b7d75d7fb"
      },
      "source": [
        "\n",
        "X = preprocessing.MinMaxScaler().fit_transform(X)\n",
        "print(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=.70, random_state=42)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.70833333 0.48113208]\n",
            " [0.79166667 0.62264151]\n",
            " [0.79166667 0.24528302]\n",
            " [0.16666667 0.33962264]\n",
            " [0.25       0.33962264]\n",
            " [0.5625     0.24528302]\n",
            " [0.6875     0.43396226]\n",
            " [0.58333333 0.24528302]\n",
            " [0.70833333 0.33962264]\n",
            " [0.5        0.43396226]\n",
            " [0.58333333 0.43396226]\n",
            " [0.5625     0.43396226]\n",
            " [0.5625     0.33962264]\n",
            " [0.3125     0.24528302]\n",
            " [0.47916667 0.73584906]\n",
            " [0.58333333 0.52830189]\n",
            " [0.39583333 0.1509434 ]\n",
            " [0.52083333 0.43396226]\n",
            " [0.39583333 0.33962264]\n",
            " [0.41666667 0.33962264]\n",
            " [0.72916667 0.1509434 ]\n",
            " [0.60416667 0.52830189]\n",
            " [0.60416667 0.24528302]\n",
            " [0.60416667 0.35849057]\n",
            " [0.64583333 0.33962264]\n",
            " [0.4375     0.24528302]\n",
            " [0.60416667 0.24528302]\n",
            " [0.77083333 0.52830189]\n",
            " [0.29166667 0.52830189]\n",
            " [0.22916667 0.1509434 ]\n",
            " [0.83333333 0.43396226]\n",
            " [0.64583333 0.21698113]\n",
            " [0.72916667 0.43396226]\n",
            " [0.625      0.38679245]\n",
            " [0.3125     0.33962264]\n",
            " [0.27083333 0.43396226]\n",
            " [0.29166667 0.24528302]\n",
            " [0.58333333 0.52830189]\n",
            " [0.54166667 0.35849057]\n",
            " [0.66666667 0.52830189]\n",
            " [0.75       0.52830189]\n",
            " [0.22916667 0.43396226]\n",
            " [0.875      0.62264151]\n",
            " [0.625      0.52830189]\n",
            " [0.66666667 0.33962264]\n",
            " [0.60416667 0.16981132]\n",
            " [0.45833333 0.1509434 ]\n",
            " [0.4375     0.52830189]\n",
            " [0.75       0.43396226]\n",
            " [0.5        0.33962264]\n",
            " [0.25       0.10377358]\n",
            " [0.75       0.24528302]\n",
            " [0.3125     0.16981132]\n",
            " [0.3125     0.33962264]\n",
            " [0.64583333 0.33962264]\n",
            " [0.52083333 0.28301887]\n",
            " [0.4375     0.43396226]\n",
            " [0.25       0.1509434 ]\n",
            " [0.52083333 0.29245283]\n",
            " [0.45833333 0.29245283]\n",
            " [0.45833333 0.33962264]\n",
            " [0.35416667 0.45283019]\n",
            " [0.60416667 0.32075472]\n",
            " [0.52083333 0.38679245]\n",
            " [0.52083333 0.24528302]\n",
            " [0.64583333 0.48113208]\n",
            " [0.64583333 0.43396226]\n",
            " [0.52083333 0.52830189]\n",
            " [0.625      0.71698113]\n",
            " [0.35416667 0.52830189]\n",
            " [0.75       0.5754717 ]\n",
            " [0.79166667 0.29245283]\n",
            " [0.6875     0.24528302]\n",
            " [0.75       0.1509434 ]\n",
            " [0.3125     0.1509434 ]\n",
            " [0.75       0.62264151]\n",
            " [0.64583333 0.29245283]\n",
            " [0.45833333 0.43396226]\n",
            " [0.39583333 0.33962264]\n",
            " [0.60416667 0.52830189]\n",
            " [0.33333333 0.09433962]\n",
            " [0.5        0.33962264]\n",
            " [0.20833333 0.43396226]\n",
            " [0.8125     0.81132075]\n",
            " [0.47916667 0.24528302]\n",
            " [0.3125     0.43396226]\n",
            " [0.375      0.41509434]\n",
            " [0.5        0.32075472]\n",
            " [0.5        0.41509434]\n",
            " [0.45833333 0.33962264]\n",
            " [0.77083333 0.24528302]\n",
            " [0.6875     0.62264151]\n",
            " [0.6875     0.33962264]\n",
            " [0.3125     0.13207547]\n",
            " [0.70833333 0.38679245]\n",
            " [0.47916667 0.32075472]\n",
            " [0.625      0.1509434 ]\n",
            " [0.64583333 0.52830189]\n",
            " [0.47916667 0.37735849]\n",
            " [0.39583333 0.26415094]\n",
            " [0.33333333 0.19811321]\n",
            " [0.10416667 0.22641509]\n",
            " [0.58333333 0.32075472]\n",
            " [0.875      0.1509434 ]\n",
            " [0.41666667 0.24528302]\n",
            " [0.52083333 0.13207547]\n",
            " [0.625      0.43396226]\n",
            " [0.58333333 0.32075472]\n",
            " [0.66666667 0.24528302]\n",
            " [0.20833333 0.22641509]\n",
            " [0.66666667 0.48113208]\n",
            " [0.5625     0.29245283]\n",
            " [0.47916667 0.22641509]\n",
            " [0.29166667 0.35849057]\n",
            " [0.6875     0.33962264]\n",
            " [0.25       0.38679245]\n",
            " [0.60416667 0.43396226]\n",
            " [0.125      0.41509434]\n",
            " [0.70833333 0.33962264]\n",
            " [0.75       0.38679245]\n",
            " [0.39583333 0.33962264]\n",
            " [0.70833333 0.52830189]\n",
            " [0.45833333 0.05660377]\n",
            " [0.54166667 0.43396226]\n",
            " [0.75       0.41509434]\n",
            " [0.33333333 0.33962264]\n",
            " [0.5625     1.        ]\n",
            " [0.52083333 0.1509434 ]\n",
            " [0.3125     0.24528302]\n",
            " [0.6875     0.28301887]\n",
            " [0.52083333 0.24528302]\n",
            " [0.45833333 0.        ]\n",
            " [0.         0.33962264]\n",
            " [0.45833333 0.43396226]\n",
            " [0.29166667 0.26415094]\n",
            " [0.54166667 0.38679245]\n",
            " [0.85416667 0.48113208]\n",
            " [0.6875     0.24528302]\n",
            " [0.125      0.24528302]\n",
            " [0.45833333 0.29245283]\n",
            " [0.625      0.43396226]\n",
            " [0.625      0.71698113]\n",
            " [0.47916667 0.32075472]\n",
            " [0.72916667 0.29245283]\n",
            " [0.60416667 0.10377358]\n",
            " [0.375      0.13207547]\n",
            " [0.58333333 0.66981132]\n",
            " [0.25       0.16981132]\n",
            " [0.33333333 0.32075472]\n",
            " [0.64583333 0.0754717 ]\n",
            " [0.47916667 0.54716981]\n",
            " [0.27083333 0.0754717 ]\n",
            " [0.79166667 0.19811321]\n",
            " [0.54166667 0.62264151]\n",
            " [0.72916667 0.24528302]\n",
            " [0.85416667 0.33962264]\n",
            " [0.45833333 0.43396226]\n",
            " [0.60416667 0.29245283]\n",
            " [0.64583333 0.43396226]\n",
            " [0.8125     0.22641509]\n",
            " [0.35416667 0.06603774]\n",
            " [1.         0.29245283]\n",
            " [0.52083333 0.1509434 ]\n",
            " [0.60416667 0.05660377]\n",
            " [0.39583333 0.28301887]\n",
            " [0.58333333 0.35849057]\n",
            " [0.47916667 0.41509434]\n",
            " [0.52083333 0.35849057]\n",
            " [0.125      0.30188679]\n",
            " [0.33333333 0.16981132]\n",
            " [0.85416667 0.62264151]\n",
            " [0.5        0.45283019]\n",
            " [0.625      0.75471698]\n",
            " [0.6875     0.43396226]\n",
            " [0.72916667 0.48113208]\n",
            " [0.58333333 0.54716981]\n",
            " [0.47916667 0.13207547]\n",
            " [0.5625     0.35849057]\n",
            " [0.29166667 0.33962264]\n",
            " [0.5        0.33962264]\n",
            " [0.39583333 0.28301887]\n",
            " [0.5625     0.37735849]\n",
            " [0.27083333 0.50943396]\n",
            " [0.625      0.79245283]\n",
            " [0.64583333 0.60377358]\n",
            " [0.70833333 0.43396226]\n",
            " [0.27083333 0.24528302]\n",
            " [0.77083333 0.62264151]\n",
            " [0.52083333 0.9245283 ]\n",
            " [0.83333333 0.43396226]\n",
            " [0.4375     0.33018868]\n",
            " [0.45833333 0.43396226]\n",
            " [0.29166667 0.35849057]\n",
            " [0.6875     0.41509434]\n",
            " [0.8125     0.24528302]\n",
            " [0.79166667 0.05660377]\n",
            " [0.83333333 0.62264151]\n",
            " [0.33333333 0.41509434]\n",
            " [0.4375     0.24528302]\n",
            " [0.625      0.62264151]\n",
            " [0.4375     0.1509434 ]\n",
            " [0.72916667 0.81132075]\n",
            " [0.58333333 0.52830189]\n",
            " [0.72916667 0.43396226]\n",
            " [0.29166667 0.1509434 ]\n",
            " [0.33333333 0.45283019]\n",
            " [0.60416667 0.32075472]\n",
            " [0.4375     0.47169811]\n",
            " [0.54166667 0.33962264]\n",
            " [0.6875     0.52830189]\n",
            " [0.16666667 0.24528302]\n",
            " [0.1875     0.24528302]\n",
            " [0.25       0.33962264]\n",
            " [0.77083333 0.79245283]\n",
            " [0.47916667 0.16981132]\n",
            " [0.5625     0.24528302]\n",
            " [0.35416667 0.10377358]\n",
            " [0.35416667 0.41509434]\n",
            " [0.72916667 0.33962264]\n",
            " [0.625      0.41509434]\n",
            " [0.25       0.16981132]\n",
            " [0.52083333 0.13207547]\n",
            " [0.20833333 0.        ]\n",
            " [0.5        0.27358491]\n",
            " [0.70833333 0.13207547]\n",
            " [0.10416667 0.22641509]\n",
            " [0.375      0.16981132]\n",
            " [0.79166667 0.54716981]\n",
            " [0.52083333 0.1509434 ]\n",
            " [0.77083333 0.16981132]\n",
            " [0.47916667 0.39622642]\n",
            " [0.54166667 0.81132075]\n",
            " [0.41666667 0.22641509]\n",
            " [0.9375     0.24528302]\n",
            " [0.52083333 0.62264151]\n",
            " [0.52083333 0.26415094]\n",
            " [0.5625     0.33962264]\n",
            " [0.35416667 0.24528302]\n",
            " [0.41666667 0.37735849]\n",
            " [0.27083333 0.24528302]\n",
            " [0.25       0.1509434 ]\n",
            " [0.25       0.30188679]\n",
            " [0.41666667 0.33962264]\n",
            " [0.66666667 0.37735849]\n",
            " [0.64583333 0.24528302]\n",
            " [0.79166667 0.24528302]\n",
            " [0.60416667 0.05660377]\n",
            " [0.375      0.1509434 ]\n",
            " [0.47916667 0.29245283]\n",
            " [0.6875     0.32075472]\n",
            " [0.58333333 0.1509434 ]\n",
            " [0.60416667 0.49056604]\n",
            " [0.72916667 0.32075472]\n",
            " [0.45833333 0.24528302]\n",
            " [0.29166667 0.19811321]\n",
            " [0.27083333 0.24528302]\n",
            " [0.79166667 0.11320755]\n",
            " [0.97916667 0.43396226]\n",
            " [0.85416667 0.58490566]\n",
            " [0.58333333 0.28301887]\n",
            " [0.3125     0.22641509]\n",
            " [0.60416667 0.39622642]\n",
            " [0.64583333 0.52830189]\n",
            " [0.3125     0.24528302]\n",
            " [0.66666667 0.41509434]\n",
            " [0.27083333 0.39622642]\n",
            " [0.47916667 0.32075472]\n",
            " [0.625      0.30188679]\n",
            " [0.22916667 0.54716981]\n",
            " [0.27083333 0.33962264]\n",
            " [0.66666667 0.43396226]\n",
            " [0.77083333 0.62264151]\n",
            " [0.35416667 0.43396226]\n",
            " [0.875      0.16981132]\n",
            " [0.625      0.37735849]\n",
            " [0.72916667 0.71698113]\n",
            " [0.77083333 0.49056604]\n",
            " [0.20833333 0.41509434]\n",
            " [0.58333333 0.56603774]\n",
            " [0.60416667 0.33962264]\n",
            " [0.58333333 0.1509434 ]\n",
            " [0.375      0.33962264]\n",
            " [0.54166667 0.32075472]\n",
            " [0.125      0.26415094]\n",
            " [0.66666667 0.50943396]\n",
            " [0.60416667 0.18867925]\n",
            " [0.60416667 0.71698113]\n",
            " [0.60416667 0.29245283]\n",
            " [0.5625     0.33962264]\n",
            " [0.5625     0.24528302]\n",
            " [0.79166667 0.54716981]\n",
            " [0.54166667 0.35849057]\n",
            " [0.3125     0.24528302]\n",
            " [0.70833333 0.43396226]\n",
            " [0.70833333 0.28301887]\n",
            " [0.25       0.24528302]\n",
            " [0.625      0.66037736]\n",
            " [0.58333333 0.43396226]\n",
            " [0.33333333 0.1509434 ]\n",
            " [0.8125     0.47169811]\n",
            " [0.58333333 0.33962264]\n",
            " [0.58333333 0.33962264]\n",
            " [0.1875     0.41509434]]\n",
            "(212, 2)\n",
            "(91, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_bFpg7282XK"
      },
      "source": [
        "# **Defining SMO Fuctions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNII4Edy7-oa"
      },
      "source": [
        "def Sequential____Minimal____Optimization(data, label, C = 1.0, kernel_type = 'linear'):\n",
        "    Sequential____Minimal____Optimization_______diff = IMORTED____lib___numpy.inf\n",
        "    epsilon = 0.001\n",
        "    samples, features = data.shape\n",
        "    Sequential____Minimal____Optimi_____alpha = IMORTED____lib___numpy.zeros(samples)\n",
        "    if kernel_type == 'linear':\n",
        "        kernel = Sequential____Minimal____Optimization____linear_kernel\n",
        "    if kernel_type == 'polynomial':\n",
        "        kernel = Sequential____Minimal____Optimization____polynomial_kernel\n",
        "    if kernel_type == 'rbf':\n",
        "        kernel = Sequential____Minimal____Optimization____rbf_kernel\n",
        "    \n",
        "    \n",
        "    \n",
        "    current_iteration = 0\n",
        "    W = 0\n",
        "    B = 0\n",
        "    TOTAL________NUM______passes = 0\n",
        "    max_passes = 100\n",
        "    tol = 0.0001\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    while TOTAL________NUM______passes < max_passes:\n",
        "        current_iteration += 1\n",
        "        alpha_old = IMORTED____lib___numpy.copy(Sequential____Minimal____Optimi_____alpha)\n",
        "        changes = 0\n",
        "        for i in range(samples):\n",
        "            W = data.T@(alpha_old*label)\n",
        "            Error_i = IMORTED____lib___numpy.sign(W.T@data[i] + B) - label[i]\n",
        "            if not (((label[i]*Error_i < -tol) and (Sequential____Minimal____Optimi_____alpha[i] < C)) or ((label[i]*Error_i > tol))):\n",
        "                continue\n",
        "            j = i\n",
        "            while j == i:\n",
        "                j = random.randint(0, samples-1)\n",
        "            eta = 2 * kernel(data[i], data[j]) - kernel(data[i], data[i]) - kernel(data[j], data[j])\n",
        "            if eta == 0:\n",
        "                continue\n",
        "            if(label[i] != label[j]):\n",
        "                L = max(0, Sequential____Minimal____Optimi_____alpha[j] - Sequential____Minimal____Optimi_____alpha[i])\n",
        "                H = min(C, C - Sequential____Minimal____Optimi_____alpha[i] + Sequential____Minimal____Optimi_____alpha[j])\n",
        "            else:\n",
        "                L = max(0, Sequential____Minimal____Optimi_____alpha[i] + Sequential____Minimal____Optimi_____alpha[j] - C)\n",
        "                H = min(C, Sequential____Minimal____Optimi_____alpha[i] + Sequential____Minimal____Optimi_____alpha[j])\n",
        "            if L == H:\n",
        "                continue\n",
        "            \n",
        "            \n",
        "            \n",
        "            Sequential____Minimal____Optimization__Error_j = IMORTED____lib___numpy.sign(W.T@data[j] + B) - label[j]\n",
        "            Sequential____Minimal____Optimi_____alpha[j] = alpha_old[j] - label[j] * (Error_i - Sequential____Minimal____Optimization__Error_j)/ eta\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n",
        "            if Sequential____Minimal____Optimi_____alpha[j] > H:\n",
        "                Sequential____Minimal____Optimi_____alpha[j] = H\n",
        "            if Sequential____Minimal____Optimi_____alpha[j] < L:\n",
        "                Sequential____Minimal____Optimi_____alpha[j] = L\n",
        "            if abs(Sequential____Minimal____Optimi_____alpha[j] - alpha_old[j]) < 0.00001:\n",
        "                Sequential____Minimal____Optimi_____alpha[j] = alpha_old[j]\n",
        "                continue\n",
        "            Sequential____Minimal____Optimi_____alpha[i] = alpha_old[i] + label[i]*label[j] * (alpha_old[j] - Sequential____Minimal____Optimi_____alpha[j])\n",
        "            b1 = B - Error_i - label[i]*(Sequential____Minimal____Optimi_____alpha[i] - alpha_old[i]) * kernel(data[i], data[i]) - label[j]*(Sequential____Minimal____Optimi_____alpha[j] - alpha_old[j]) * kernel(data[i], data[j])\n",
        "            b2 = B - Sequential____Minimal____Optimization__Error_j - label[i]*(Sequential____Minimal____Optimi_____alpha[i] - alpha_old[i]) * kernel(data[i], data[j]) - label[j]*(Sequential____Minimal____Optimi_____alpha[j] - alpha_old[j]) * kernel(data[j], data[j])\n",
        "            if Sequential____Minimal____Optimi_____alpha[i] > 0 and Sequential____Minimal____Optimi_____alpha[i] < C:\n",
        "                B = b1\n",
        "            elif Sequential____Minimal____Optimi_____alpha[j] > 0 and Sequential____Minimal____Optimi_____alpha[j] < C:\n",
        "                B = b2\n",
        "            else:\n",
        "                B = (b1 + b2) / 2\n",
        "            changes += 1\n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        \n",
        "        if changes == 0:\n",
        "            TOTAL________NUM______passes += 1\n",
        "        else:\n",
        "            TOTAL________NUM______passes = 0\n",
        "\n",
        "        \n",
        "        \n",
        "        Sequential____Minimal____Optimization_______diff = IMORTED____lib___numpy.linalg.norm(Sequential____Minimal____Optimi_____alpha - alpha_old)\n",
        "        \n",
        "        print('Iteration : ', current_iteration , ', Difference : ' , Sequential____Minimal____Optimization_______diff, ', Changes : ', changes)\n",
        "    \n",
        "    \n",
        "    W = data.T@(Sequential____Minimal____Optimi_____alpha * label)\n",
        "    B = IMORTED____lib___numpy.mean(label - W.T@data.T)\n",
        "    return W, B\n",
        "\n",
        "def Sequential____Minimal____Optimization____linear_kernel(x1, x2):\n",
        "    return x1@x2.T\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Sequential____Minimal____Optimization____polynomial_kernel(x1, x2):\n",
        "    return (x1@x2.T+1)**2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Sequential____Minimal____Optimization____rbf_kernel(x1, x2, sigma = 1):\n",
        "    return IMORTED____lib___numpy.exp(-IMORTED____lib___numpy.linalg.norm(x1 - x2) ** 2 / (2 * (sigma ** 2)))\n",
        "\n"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7HpY9T297hK"
      },
      "source": [
        "#Defining Accuracy Functions and Predicting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddrndTeJ7-rR",
        "outputId": "792c228b-6d62-41de-ce92-d188d282246a"
      },
      "source": [
        "def predict(data, W, B, total__________var__________threshhold = 0):\n",
        "    if total__________var__________threshhold == 0:\n",
        "        return IMORTED____lib___numpy.sign(W.T@data.T + B)\n",
        "    final_________________result = W.T@data.T + B\n",
        "    ret = []\n",
        "    for THRESHOLD_________________val in final_________________result:\n",
        "        if THRESHOLD_________________val >= total__________var__________threshhold:\n",
        "            ret.append(1)\n",
        "        else:\n",
        "            ret.append(-1)\n",
        "    return IMORTED____lib___numpy.array(ret)\n",
        "\n",
        "W, B = Sequential____Minimal____Optimization(X_train, y_train, C = 1, kernel_type='polynomial')\n",
        "\n",
        "def Sequential____Minimal____Optimization_____Accuracy(actual, predict):\n",
        "    correct = 0\n",
        "    for i in range(len(actual)):\n",
        "        if actual[i] == predict[i]:\n",
        "            correct += 1\n",
        "    return correct / len(actual) * 100\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Y_predict = predict(X_test, W, B)\n",
        "print(Sequential____Minimal____Optimization_____Accuracy(y_test, Y_predict))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def Fetching____________TPRandFPR(predicted, actual):\n",
        "    confusion_matrix = IMORTED____lib___numpy.zeros((2,2))\n",
        "    for i in range(len(predicted)):\n",
        "        if(actual[i] == -1):\n",
        "            if predicted[i] == -1:\n",
        "                confusion_matrix[0][0] += 1\n",
        "            if predicted[i] == 1:\n",
        "                confusion_matrix[0][1] += 1\n",
        "        else:\n",
        "            if predicted[i] == -1:\n",
        "                confusion_matrix[1][0] += 1\n",
        "            if predicted[i] == 1:\n",
        "                confusion_matrix[1][1] += 1\n",
        "    TOTAL____NUM____OF___TRUE___PR = confusion_matrix[1][1] / (confusion_matrix[1][1] + confusion_matrix[1][0])\n",
        "    TOTAL____NUM____OF___FALSE___PR = confusion_matrix[0][1] / (confusion_matrix[0][1] + confusion_matrix[0][0])\n",
        "    return TOTAL____NUM____OF___TRUE___PR, TOTAL____NUM____OF___FALSE___PR\n"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration :  1 , Difference :  8.18535277187245 , Changes :  40\n",
            "Iteration :  2 , Difference :  5.914664213382954 , Changes :  21\n",
            "Iteration :  3 , Difference :  5.868872228843154 , Changes :  19\n",
            "Iteration :  4 , Difference :  4.714000083959629 , Changes :  12\n",
            "Iteration :  5 , Difference :  3.1447957583317567 , Changes :  7\n",
            "Iteration :  6 , Difference :  3.219974876653824 , Changes :  8\n",
            "Iteration :  7 , Difference :  3.050879777562697 , Changes :  5\n",
            "Iteration :  8 , Difference :  3.4641016151377544 , Changes :  6\n",
            "Iteration :  9 , Difference :  1.4142135623730951 , Changes :  1\n",
            "Iteration :  10 , Difference :  2.7046828255865756 , Changes :  5\n",
            "Iteration :  11 , Difference :  2.054701144101347 , Changes :  3\n",
            "Iteration :  12 , Difference :  2.0 , Changes :  2\n",
            "Iteration :  13 , Difference :  0.5431551333887922 , Changes :  2\n",
            "Iteration :  14 , Difference :  1.9440886613636785 , Changes :  3\n",
            "Iteration :  15 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  16 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  17 , Difference :  1.8187543586589727 , Changes :  2\n",
            "Iteration :  18 , Difference :  1.4398682951420287 , Changes :  2\n",
            "Iteration :  19 , Difference :  1.4142135623730951 , Changes :  1\n",
            "Iteration :  20 , Difference :  1.5800909677465598 , Changes :  2\n",
            "Iteration :  21 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  22 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  23 , Difference :  1.4142135623730951 , Changes :  1\n",
            "Iteration :  24 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  25 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  26 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  27 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  28 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  29 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  30 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  31 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  32 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  33 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  34 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  35 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  36 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  37 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  38 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  39 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  40 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  41 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  42 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  43 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  44 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  45 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  46 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  47 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  48 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  49 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  50 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  51 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  52 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  53 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  54 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  55 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  56 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  57 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  58 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  59 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  60 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  61 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  62 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  63 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  64 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  65 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  66 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  67 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  68 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  69 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  70 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  71 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  72 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  73 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  74 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  75 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  76 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  77 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  78 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  79 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  80 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  81 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  82 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  83 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  84 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  85 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  86 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  87 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  88 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  89 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  90 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  91 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  92 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  93 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  94 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  95 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  96 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  97 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  98 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  99 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  100 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  101 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  102 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  103 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  104 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  105 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  106 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  107 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  108 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  109 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  110 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  111 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  112 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  113 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  114 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  115 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  116 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  117 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  118 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  119 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  120 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  121 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  122 , Difference :  0.0 , Changes :  0\n",
            "Iteration :  123 , Difference :  0.0 , Changes :  0\n",
            "59.34065934065934\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "NXWCoM0O7-t5",
        "outputId": "b86b6ba2-c6b3-4de3-f6a8-e5d19715c812"
      },
      "source": [
        "TOTAL___________threshholds = IMORTED____lib___numpy.arange(-500, 500, 2)\n",
        "VAR_____________X______plot_____ = []\n",
        "VAR_____________Y______plot = []\n",
        "for THRESHOLD_________________val in TOTAL___________threshholds:\n",
        "    TOTAL____NUM____OF___TRUE___PR, TOTAL____NUM____OF___FALSE___PR =Fetching____________TPRandFPR(predict(X_test, W, B, THRESHOLD_________________val),y_test)\n",
        "    VAR_____________X______plot_____.append(TOTAL____NUM____OF___FALSE___PR)\n",
        "    VAR_____________Y______plot.append(TOTAL____NUM____OF___TRUE___PR)\n",
        "\n",
        "IMORTED____lib___plot.figure(figsize=(8,8))\n",
        "IMORTED____lib___plot.title('ROC curve')\n",
        "IMORTED____lib___plot.xlabel('False Positive Rate')\n",
        "IMORTED____lib___plot.ylabel('True Positive Rate')\n",
        "IMORTED____lib___plot.plot(VAR_____________X______plot_____,VAR_____________Y______plot)\n",
        "\n"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f6d05977b10>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyXz6veb-aML"
      },
      "source": [
        ""
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}