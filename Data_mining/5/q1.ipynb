{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Naive Bayes Algorithm?\n",
    "On Summarizing the above mentioned points Naive Bayes algorithm can be defined as a supervised classification algorithm which is based on Bayes theorem with an assumption of independence among features.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bayes Theorem helps us to find the probability of a hypothesis given our prior knowledge.\n",
    "As per wikipedia,In probability theory and statistics, Bayes’ theorem (alternatively Bayes’ law or Bayes’ rule, also written as Bayes’s theorem) describes the probability of an event, based on prior knowledge of conditions that might be related to the event.\n",
    "Lets look at the equation for Bayes Theorem,\n",
    "\n",
    "Where,\n",
    "* P(A|B) is the probability of hypothesis A given the data B. This is called the posterior probability.\n",
    "* P(B|A) is the probability of data B given that the hypothesis A was true.\n",
    "* P(A) is the probability of hypothesis A being true (regardless of the data). This is called the prior probability of A.\n",
    "* P(B) is the probability of the data (regardless of the hypothesis).\n",
    "\n",
    "If you are thinking what is P(A|B) or P(B|A)?These are conditional probabilities having formula :\n",
    "\n",
    "\n",
    "# P(B|A)= P(A and B) / P(A)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Reference:https://medium.com/@srishtisawla/introduction-to-naive-bayes-for-classification-baefefb43a2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''BAYESIAN CLASSIFICATION - NAIVE BAYES CLASSIFIER'''\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Naive_Bayesian_Class(object):\n",
    "    '''NAIVE BAYES CLASSIFIER CLASS'''\n",
    "    \n",
    "    def __init__(self, X, Y):\n",
    "        self.labels = np.unique(Y)\n",
    "        self.features =X.shape[1]\n",
    "        self.likelihoods = self.initialize_likelihoods_parameters()\n",
    "        self.class_probabilities = self.get_probability(Y) # PRIOR PROBABILITIES\n",
    "        self.training_the_algorithm(X, Y)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def get_probability(outcome):\n",
    "        '''RETURNS A DICTIONARY WITH PROBABILITIES OF THE OCCURENCES'''\n",
    "        total_num_samples = len(outcome)\n",
    "        all_probabilities = dict(Counter(outcome))\n",
    "        for key in all_probabilities.keys():\n",
    "            all_probabilities[key] /= total_num_samples\n",
    "        return all_probabilities\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    @staticmethod\n",
    "    def creating_every_possible_subset(X, Y, label):\n",
    "        '''CREATES SUBSET OF X BELONGING TO A PARTICULAR CLASS'''\n",
    "        row_indices = np.where(Y == label)[0]\n",
    "        return X[row_indices, :]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    @staticmethod\n",
    "    def get_max_value_key(dictionary):\n",
    "        '''RETURNS DICTIONARY KEY THAT HAS THE MAXIMUM VALUE'''\n",
    "        return max(dictionary, key=dictionary.get)\n",
    "\n",
    "    \n",
    "    \n",
    "    def initialize_likelihoods_parameters(self):\n",
    "        return dict((label, defaultdict(list)) for label in self.labels)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def training_the_algorithm(self, X, Y):\n",
    "        '''TRAINS THE CLASSIFIER'''\n",
    "\n",
    "        # COUNT THE OCCURRENCES OF THE FEATURES, CLASS-WISE\n",
    "        for label in self.labels:\n",
    "            subset_X = self.creating_every_possible_subset(X, Y, label=label)\n",
    "            for feature in range(self.features):\n",
    "                self.likelihoods[label][feature] += list(subset_X[:, feature])\n",
    "\n",
    "        # TRANSFORM THE TABLE OF COUNTS INTO TABLE OF PROBABILITIES\n",
    "        for label in self.labels:\n",
    "            for feature in range(self.features):\n",
    "                self.likelihoods[label][feature] = self.get_probability(self.likelihoods[label][feature])\n",
    "\n",
    "                \n",
    "                \n",
    "    def perform_classification_in_naive_bayesian(self, X_test):\n",
    "        '''PREDICTS CLASS LABEL FOR TEST DATA'''\n",
    "        prediction = {}\n",
    "\n",
    "        # DETERMINE CLASS PROBABILITY FOR EACH CLASS\n",
    "        for label in self.labels:\n",
    "            class_probability = self.class_probabilities[label]\n",
    "            for feature in range(self.features):\n",
    "                relative_feature_values = self.likelihoods[label][feature]\n",
    "                if X_test[feature] in relative_feature_values.keys():\n",
    "                    class_probability *= relative_feature_values[X_test[feature]]\n",
    "                else:\n",
    "                    class_probability = 0\n",
    "            prediction[label] = class_probability\n",
    "\n",
    "        # RETURN CLASS WITH THE MAXIMUM PROBABILITY AS THE PREDICTION\n",
    "        return self.get_max_value_key(prediction)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons of Naive Bayes Algorithm:\n",
    "# Pros :\n",
    "* It is easy to understand.\n",
    "* It can also be trained on small dataset.\n",
    "# Cons :\n",
    "* It has a ‘Zero conditional probability Problem’, for features having zero frequency the total probability also becomes zero.There are several sample correction techniques to fix this problem such as “Laplacian Correction.”\n",
    "* Another disadvantage is the very strong assumption of independence class features that it makes. It is near to impossible to find such data sets in real life.\n",
    "# Applications of Naive Bayes Algorithm :\n",
    "Naive Bayes is widely used for text classification\n",
    "Another example of Text Classification where Naive Bayes is mostly used is Spam Filtering in Emails\n",
    "Other Examples include Sentiment Analysis ,Recommender Systems etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET LENGTH\t:\t 13\n",
      "DATASET SHAPE\t:\t (13, 11)\n",
      "\n",
      "   ALT Bar Fri Hun   Pat Price Rain Res     Type       Est Target Wait\n",
      "0    T   F   F   T  Some   $$$    F   T   French   0 to 10           T\n",
      "1    T   F   F   T  Full     $    F   F     Thai  30 to 60           F\n",
      "2    F   T   F   F  Some     $    F   F   Burger   0 to 10           T\n",
      "3    T   F   T   T  Full     $    F   F     Thai  10 to 30           T\n",
      "4    T   F   T   F  Full   $$$    F   T   French      > 60           F\n",
      "5    F   T   F   T  Some    $$    T   T  Italian   0 to 10           T\n",
      "6    F   T   F   F  None     $    T   F   Burger   0 to 10           F\n",
      "7    F   F   F   T  Some    $$    T   T     Thai   0 to 10           T\n",
      "8    F   T   T   F  Full     $    T   F   Burger      > 60           F\n",
      "9    T   T   T   T  Full   $$$    F   T  Italian  10 to 30           F\n",
      "10   F   F   F   F  None     $    F   F     Thai   0 to 10           F\n",
      "11   T   T   T   T  Full     $    F   F   Burger  30 to 60           T\n",
      "12   T   T   F   T  Some   $$$    T   T  Italian      > 60           ?\n",
      "\n",
      "Target Wait\t:\tF\n"
     ]
    }
   ],
   "source": [
    "        \n",
    "        \n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "dataset = pd.read_csv('data/DATASET.csv', sep=',')\n",
    "headers = dataset.columns\n",
    "print('DATASET LENGTH\\t:\\t', len(dataset))\n",
    "print('DATASET SHAPE\\t:\\t', dataset.shape)\n",
    "print('\\n{}'.format(dataset))\n",
    "\n",
    "dataset = dataset.to_numpy()\n",
    "X_train, y_train = np.array(dataset[:-1, :-1]), np.array(dataset[:-1, -1])\n",
    "test_data = np.array(dataset[-1, :-1])\n",
    "print('\\n{}\\t:\\t{}'.format(headers[-1], Naive_Bayesian_Class(X_train, y_train).perform_classification_in_naive_bayesian(test_data)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
